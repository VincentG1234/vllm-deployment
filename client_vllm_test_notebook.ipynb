{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Client Notebook for Testing a vLLM Server\n",
        "\n",
        "This notebook allows you to **send requests to a local or remote vLLM server** (compatible with the OpenAI API format) and test its performance and capabilities.\n",
        "\n",
        "You will learn to:\n",
        "- Run simple and streaming completions using raw HTTP and the `openai` library.\n",
        "- Benchmark the server's latency and performance under load.\n",
        "\n",
        "![server vLLM](images/schema_client_server.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Initial Setup\n",
        "\n",
        "Before we can send requests, we need to configure a few key parameters. This section sets up the server connection details and the prompt we'll use for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Server Connection Details\n",
        "\n",
        "You will access the vLLM server over the network. Please enter the public IP address of your EC2 instance where the server is running. If you have configured a different port during the server launch, please update the `PORT` variable as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The public IP address of the EC2 instance running the vLLM server.\n",
        "# ‚û°Ô∏è ACTION: Replace the placeholder with your actual public IP.\n",
        "PUBLIC_IP = \"XYZ\"  # TODO: Replace with your actual public IP\n",
        "\n",
        "# The port the vLLM server is listening on. The default is 8000.\n",
        "PORT = \"8000\"\n",
        "\n",
        "# The model name to be used for the requests.\n",
        "# This must match the model name that the vLLM server has loaded.\n",
        "# For servers launched with a model like 'mistralai/Mistral-7B-v0.2', the name is often the same.\n",
        "MODEL_NAME = \"mistral-7b\"  # TODO set the right model name\n",
        "\n",
        "# Construct the base URL for the API endpoints.\n",
        "URL = f\"http://{PUBLIC_IP}:{PORT}\"\n",
        "\n",
        "print(f\"Will connect to server at: {URL}\")\n",
        "print(f\"Will use model: '{MODEL_NAME}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Define Your Prompt\n",
        "\n",
        "Now, let's define the prompt we will send to the model. You can change this to any question you like. Keep in mind the capabilities of the model you are using (e.g., a 7B model is powerful, but not at the level of GPT-4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You can modify the prompt to ask the model anything you want.\n",
        "PROMPT = \"\"\"\n",
        "Explique la loi de l'offre et de la demande en √©conomie, en donnant un exemple simple.\n",
        "\"\"\"  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Completion using a Raw HTTP Request\n",
        "\n",
        "First, let's interact with the server at a low level using the `requests` library. This helps us understand the raw API structure, which is compliant with the OpenAI Chat Completions API format. We are sending a POST request to the `/v1/chat/completions` endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Define the headers for our HTTP request. We specify the content type is JSON.\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Define the payload (body) of our request.\n",
        "data = {\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": PROMPT}\n",
        "    ],\n",
        "    \"max_tokens\": 500,\n",
        "}\n",
        "\n",
        "# The full URL for the chat completions endpoint.\n",
        "url_completions = URL + \"/v1/chat/completions\"\n",
        "\n",
        "# Send the POST request.\n",
        "try:\n",
        "    response = requests.post(url_completions, headers=headers, json=data, timeout=60)\n",
        "    print(\"Status code:\", response.status_code)\n",
        "    print(\"\\n\" + \"-\"*100 + \"\\n\")\n",
        "    resp_json = response.json()\n",
        "    print(\"Full Response JSON:\")\n",
        "    print(json.dumps(resp_json, indent=2, ensure_ascii=False))\n",
        "    print(\"\\n\" + \"-\"*100 + \"\\n\")\n",
        "    print(\"Generated Content:\")\n",
        "    print(resp_json[\"choices\"][0][\"message\"][\"content\"])\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(\"HTTP Request failed:\", e)\n",
        "except Exception as e:\n",
        "    print(\"Error reading the response:\", e)\n",
        "    if 'response' in locals():\n",
        "        print(\"Raw Response Text:\", response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Using the OpenAI Python Library\n",
        "\n",
        "While using `requests` is good for understanding the underlying API, it's more convenient to use a dedicated library. The `openai` library can be configured to communicate with any OpenAI-compatible API, including our vLLM server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "# Initialize the OpenAI client.\n",
        "client = OpenAI(\n",
        "    base_url=f\"{URL}/v1\",\n",
        "    api_key=\"EMPTY\"  # vLLM n'utilise pas la cl√© mais c'est requis par le client\n",
        ")\n",
        "\n",
        "print(\"OpenAI client configured to connect to:\", client.base_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîÅ Simple Completion Request\n",
        "\n",
        "This is the most basic type of request. We send the prompt and wait for the full response to be generated before it's returned to us. This is also known as a blocking request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Sending a simple completion request for model '{MODEL_NAME}'...\\n\")\n",
        "\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": PROMPT}\n",
        "        ]\n",
        "    )\n",
        "    print(response.choices[0].message.content)\n",
        "except Exception as e:\n",
        "    print(f\"Error during OpenAI completion: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üì° Streaming Completion Request\n",
        "\n",
        "For a better user experience, especially with long responses, we can stream the output. This means the server sends back the response token by token as it's being generated, rather than waiting for the entire sequence to be complete. This makes the application feel much more responsive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Sending a streaming request for model '{MODEL_NAME}'...\\n\")\n",
        "\n",
        "try:\n",
        "    stream = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[{\"role\": \"user\", \"content\": PROMPT}],\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    output = \"\"\n",
        "    print(\"--- Streaming Response ---\")\n",
        "    for chunk in stream:\n",
        "        if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:\n",
        "            content = chunk.choices[0].delta.content\n",
        "            print(content, end=\"\", flush=True)\n",
        "            output += content\n",
        "    print(\"\\n--- End of Stream ---\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during streaming completion: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ‚è±Ô∏è concurrent requests performance\n",
        "\n",
        "Finally, let's measure the performance of our vLLM server. We will send multiple requests concurrently to simulate a real-world load and measure the latency of each request. This helps us understand the server's throughput and how well it can handle parallel requests.\n",
        "\n",
        "We will measure:\n",
        "- **Latency**: The time it takes to get a response for a single token (in seconds).\n",
        "- **Throughput**: The number of requests the server can handle per second."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import numpy as np\n",
        "\n",
        "def single_request(prompt: str):\n",
        "    \"\"\"Sends a single request to the LLM and returns its latency.\"\"\"\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        client.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        end_time = time.time()\n",
        "        latency = end_time - start_time\n",
        "        return latency\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred in a request: {e}\")\n",
        "        return None\n",
        "\n",
        "def run_benchmark(n_requests: int = 10, prompt: str = PROMPT):\n",
        "    \"\"\"Runs a benchmark by sending n_requests concurrently.\"\"\"\n",
        "    print(f\"Starting benchmark with {n_requests} concurrent requests...\\n\")\n",
        "    latencies = []\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_requests) as executor:\n",
        "        futures = [executor.submit(single_request, prompt) for _ in range(n_requests)]\n",
        "        for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
        "            latency = future.result()\n",
        "            if latency is not None:\n",
        "                latencies.append(latency)\n",
        "                print(f\"Request {i+1} completed in {latency:.2f}s\")\n",
        "\n",
        "    if not latencies:\n",
        "        print(\"\\nNo requests completed successfully.\")\n",
        "        return\n",
        "\n",
        "    avg_latency = np.mean(latencies)\n",
        "    min_latency = np.min(latencies)\n",
        "    max_latency = np.max(latencies)\n",
        "    \n",
        "\n",
        "    print(\"\\n=== Benchmark Summary ===\")\n",
        "    print(f\"Total Successful Requests: {len(latencies)}\")\n",
        "    print(f\"Average Latency: {avg_latency:.2f}s\")\n",
        "    print(f\"Min Latency: {min_latency:.2f}s\")\n",
        "    print(f\"Max Latency: {max_latency:.2f}s\")\n",
        "\n",
        "# Run the benchmark with 10 requests.\n",
        "run_benchmark(n_requests=10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
